# Demo de Tokenizaci√≥n: Word / Char / Subword

Este proyecto muestra, en consola, c√≥mo una misma frase se tokeniza con tres enfoques distintos:

- **Word-based** (NLTK)
- **Character-based** (car√°cter a car√°cter)
- **Subword-based** (BERT WordPiece) + **tokens especiales** + **vocabulario** + **IDs** (+ padding demostrativo)

Est√° pensado para acompa√±ar el art√≠culo de LinkedIn: *‚ÄúTokenizaci√≥n: el idioma que entienden los LLMs‚Äù*.

---

##  1) Preparar entorno virtual

### Opci√≥n A ¬∑ `venv` (recomendada)
#### Windows (PowerShell)
```powershell
# En la carpeta del proyecto
py -m venv .venv
.\.venv\Scripts\Activate.ps1

# Actualiza pip (opcional pero recomendado)
python -m pip install --upgrade pip
```

#### Linux / macOS (bash/zsh)
```bash
# En la carpeta del proyecto
python3 -m venv .venv
source .venv/bin/activate

# Actualiza pip (opcional)
python -m pip install --upgrade pip
```

> Para salir del entorno virtual: `deactivate`

### Opci√≥n B ¬∑ Conda (alternativa)
```bash
conda create -n nlp311 python=3.11 -y
conda activate nlp311
```

---

##  2) Instalar dependencias con `requirements.txt`

Crea un archivo `requirements.txt` con el siguiente contenido (CPU‚Äëonly, versiones compatibles entre s√≠):

```txt
torch==2.2.2
torchtext==0.17.2
transformers==4.42.1
sentencepiece
nltk
```

Instala:

```bash
pip install -r requirements.txt
```

> üí° La primera vez que ejecutes el script, `transformers` descargar√° autom√°ticamente el tokenizador de BERT (WordPiece).  
> üí° Si NLTK pide recursos, el script intentar√° descargar `punkt` autom√°ticamente. Si tu red bloquea descargas, consulta ‚ÄúSoluci√≥n de problemas‚Äù abajo.

---

##  3) Ejecutar

```bash
python tokenizacion.py
```

1) La pantalla se limpia y ver√°s un **prompt**:
```
 Demo de tokenizaci√≥n (Word / Char / Subword)

Escribe una frase y pulsa Enter:
```

2) Escribe tu frase (en espa√±ol o ingl√©s) y pulsa Enter.

3) Ver√°s tres bloques con la tokenizaci√≥n y un resumen final.

---

##  4) ¬øQu√© ver√°s en la salida?

### A) Word-based (NLTK)
- Divide por **palabras y puntuaci√≥n**.
- Preserva el significado sem√°ntico b√°sico.
- Limitaci√≥n: vocabulario enorme y problemas con **OOV** (palabras no vistas).

**Ejemplo:**
```
Word-based (NLTK)
Tokens: ['HAy', 'que', 'entender', 'la', 'tokenizaci√≥n', 'correctamente']
```

### B) Character-based
- Cada **car√°cter** (incluidos espacios) es un token.
- Vocabulario m√≠nimo; **no hay OOV**.
- Limitaci√≥n: secuencias muy largas y poca sem√°ntica.

**Ejemplo:**
```
Character-based
Tokens: ['H','A','y',' ', 'q','u','e',' ', ...]
```

### C) Subword-based (BERT WordPiece) + especiales + vocab
- **Palabras frecuentes** se mantienen enteras.
- **Palabras raras** se dividen en **subpalabras** (`##` indica sufijo que se une al token anterior).
- Se a√±aden **tokens especiales**:
  - `<bos>` (inicio), `<eos>` (fin), `<pad>` (relleno), `<unk>` (desconocido)
- Se construye un **vocabulario** (demo local) y se muestran **IDs** (token‚Üíentero).
- *Este es el enfoque de los LLM modernos.*

**Ejemplo:**
```
Subword-based (BERT WordPiece) + especiales + vocab
Tokens: ['<bos>', 'hay', 'que', 'en', '##ten', '##der', 'la', 'token', '##iza', '##cion', 'correct', '##ament', '##e', '<eos>']
IDs   : [2, 12, 14, 11, 9, 6, 13, 15, 8, 5, 10, 4, 7, 3]

ID | Token
---+-------
0  | <unk>
1  | <pad>
2  | <bos>
3  | <eos>
... etc.
```

### D) Padding demostrativo
- Muestra c√≥mo **igualar longitudes** de secuencias con `<pad>` (√∫til para ‚Äúhacer batches‚Äù).
- Ejemplo visual de **preprocesamiento** t√≠pico antes de entrenar/inferir.

---

##  5) C√≥mo encaja con la teor√≠a

- **Word-based**: intuitivo pero con vocabulario enorme; problemas con plurales/derivaciones.
- **Character-based**: sin OOV, pero sem√°ntica d√©bil y secuencias largas.
- **Subword-based**: equilibrio ideal ‚Üí reduce OOV, mantiene significado morfol√≥gico.  
  - **BERT/WordPiece** usa `##sufijo`.  
  - Otros como **XLNet/T5** usan SentencePiece (`‚ñÅ` para espacios).

En **PyTorch/torchtext**, tras tokenizar:
1) generamos **vocabularios** (token‚ÜíID),
2) a√±adimos **tokens especiales**,
3) y opcionalmente **paddeamos** las secuencias.

---

##  6) Soluci√≥n de problemas

- **Windows / Visual C++**: si PyTorch diera errores de DLL, instala/repara  
  *Microsoft Visual C++ Redistributable 2015‚Äì2022 (x64)*.
- **NLTK pide recursos** (`punkt` / `punkt_tab`): el script descarga `punkt` autom√°ticamente.  
  Si tu red bloquea descargas, ejecuta manualmente:
  ```python
  import nltk
  nltk.download('punkt')
  # (si te lo pidiera) nltk.download('punkt_tab')
  ```
- **Descargas de modelos** (`transformers`): la primera ejecuci√≥n puede tardar m√°s; requiere conexi√≥n a internet.

---

##  7) Estructura del repo (sugerida)

```
.
‚îú‚îÄ‚îÄ tokenizacion.py     # Script principal de demo
‚îú‚îÄ‚îÄ requirements.txt    # Dependencias
‚îî‚îÄ‚îÄ README.md           # Este archivo
```
